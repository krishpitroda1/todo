import requests
from bs4 import BeautifulSoup
import pandas as pd

# Define the URL to scrape
url = "https://www.amazon.in/sk=bags&crid=2M096C61O4MLT&qid=1653308124&sprefix=ba%2Caps%2C283&ref=sr_pg_1"

# Initialize an empty list to store product data
product_data = []

# Scrape 20 pages (you can adjust this as needed)
for page in range(1, 21):
    page_url = f"{url}&page={page}"
    response = requests.get(page_url)
    soup = BeautifulSoup(response.content, "html.parser")

    # Extract product details
    products = soup.find_all("div", class_="s-result-item")
    for product in products:
        product_url = product.find("a", class_="a-link-normal").get("href")
        product_name = product.find("span", class_="a-text-normal").text.strip()
        product_price = product.find("span", class_="a-price-whole").text.strip()
        product_rating = product.find("span", class_="a-icon-alt").text.strip()
        product_reviews = product.find("span", class_="a-size-base").text.strip()

        # Append data to the list
        product_data.append({
            "Product URL": product_url,
            "Product Name": product_name,
            "Product Price": product_price,
            "Rating": product_rating,
            "Number of Reviews": product_reviews
        })

# Create a pandas DataFrame from the collected data
df = pd.DataFrame(product_data)

# Save the data to a CSV file
df.to_csv("amazon_products.csv", index=False)

print(f"Scraped {len(df)} products. Data saved to 'amazon_products.csv'.")
